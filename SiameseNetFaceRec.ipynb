{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GLOBAL VARIABLES and IMPORTS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import zipfile\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "import random\n",
    "from random import shuffle\n",
    "from string import digits\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os.path as osp\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "%matplotlib inline\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.utils.data\n",
    "\n",
    "ZIP_PATH = 'datasets/aberANDGTdb_crop.zip' #aber&GTdb_crop.zip'\n",
    "\n",
    "EXTENSION = \".jpg\"\n",
    "SEPARATOR = \"_\" # !!! Format of the dabase: name[!]_id !!!\n",
    "RATION_TRAIN_SET = 0.75\n",
    "WITH_PROFILE = False # True if both frontally and in profile people \n",
    "SAVE_MODEL = True \n",
    "DO_LEARN = True \n",
    "\n",
    "BATCH_SIZE = 16\n",
    "LEARNING_RATE = 0.001\n",
    "NUM_EPOCH = 50 \n",
    "WEIGHT_DECAY = 0.0001\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MANAGEMENT OF THE DATA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaceImage():\n",
    "    def __init__(self, path, trans_image):\n",
    "        self.path = path\n",
    "        self.trans_img = trans_image\n",
    "    \n",
    "    def isIqual(self, other_image):\n",
    "        return other_image.path == self.path "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "STILL TODO: \n",
    "    - Better management of the dataset (during the composition of triplets): control the nb of triplets that are defined \n",
    "    - Build a separated Training and a Testing sets\n",
    "    - Putting more images in the database \n",
    "\"\"\"\n",
    "class Face_DS(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, file_names=None, transform=None):\n",
    "        \n",
    "        self.transform = transforms.ToTensor() if transform is None else transform\n",
    "\n",
    "        faces_dic = {}\n",
    "        \n",
    "        with zipfile.ZipFile(ZIP_PATH, 'r') as archive:\n",
    "            \n",
    "            if file_names is None:\n",
    "                file_names = archive.namelist()\n",
    "            \n",
    "            #################################\n",
    "            # Order the picture per label \n",
    "            #################################\n",
    "            for fn in file_names: # fn = name[!]_id\n",
    "                if fn == \".DS_Store\":\n",
    "                    continue\n",
    "                    \n",
    "                label = fn.split(SEPARATOR)[0]\n",
    "                \n",
    "                ###### Check if the picture represents the person in profile ###### \n",
    "                is_profile = True if label[-1] == \"!\" else False\n",
    "\n",
    "                if not WITH_PROFILE and is_profile: \n",
    "                    next # Case where the person is in profile while we don't want this type of picture\n",
    "                \n",
    "                if WITH_PROFILE and is_profile:\n",
    "                    label = label[:-1]\n",
    "\n",
    "                formatted_image = self.transform(Image.open(BytesIO(archive.read(fn))).convert(\"RGB\"))\n",
    "                img = FaceImage(fn, formatted_image)\n",
    "\n",
    "                try: \n",
    "                    faces_dic[label].append(img)\n",
    "                except KeyError: \n",
    "                    faces_dic[label] = [img]\n",
    "                    \n",
    "        all_labels = list(faces_dic.keys())\n",
    "        nb_labels = len(all_labels)\n",
    "        \n",
    "        ########################################################################\n",
    "        # Build triplet supporting the dataset (ensures balanced classes)\n",
    "        ########################################################################\n",
    "        self.train_data = []\n",
    "        self.train_labels = []\n",
    "        self.train_not_formatted_data = []\n",
    "        \n",
    "        ################ Consider each person ##########################\n",
    "        for label, pictures_list in faces_dic.items():\n",
    "            pic_ind_pos = list(range(len(pictures_list)))\n",
    "            shuffle(pic_ind_pos)\n",
    "            labels_indexes_neg = [x for x in range(0, nb_labels) if x != all_labels.index(label)]\n",
    "\n",
    "            \n",
    "            ################ Consider each picture of the person #####################\n",
    "            for i, picture_ref in enumerate(pictures_list):\n",
    "                \n",
    "                try: \n",
    "                    if not picture_ref.isIqual(pictures_list[pic_ind_pos[-1]]):\n",
    "                        picture_positive = pictures_list[pic_ind_pos.pop()]\n",
    "                    else:\n",
    "                        picture_positive = pictures_list[pic_ind_pos.pop(-2)]\n",
    "                except IndexError: \n",
    "                    # !! Means that the current label has no remaining other picture !! \n",
    "                    break\n",
    "                \n",
    "                # Pick a random different person \n",
    "                label_neg = all_labels[random.choice(labels_indexes_neg)]\n",
    "                picture_negative = random.choice(faces_dic[label_neg]) \n",
    "                \n",
    "                self.train_not_formatted_data.append([picture_ref.path, picture_positive.path, picture_negative.path])\n",
    "\n",
    "                self.train_data.append([picture_ref.trans_img , picture_positive.trans_img , picture_negative.trans_img ]) # torch.stack is not applied because we want a list of tensors \n",
    "                self.train_labels.append([1,0])\n",
    "                \n",
    "        #self.train_data = torch.stack(self.train_data)\n",
    "        self.train_labels = torch.tensor(self.train_labels)\n",
    "        \n",
    "        #############################################\n",
    "        # Report about the quantity of data \n",
    "        #############################################\n",
    "        pictures_nbs = [len(pictures_list) for person_name, pictures_list in faces_dic.items()]\n",
    "        max_nb_pictures = max(pictures_nbs)\n",
    "        min_nb_pictures = 1 #min(pictures_nbs) # !!!! TOSOLVe: strange error: TypeError: 'list' object is not callable, while max works\n",
    "\n",
    "        print(\" ---------------- Report about the quantity of data  -------------------- \")\n",
    "        print(\"The number of different people in set is: \" + str(nb_labels))\n",
    "        print(\"The number of pictures per person is between: \" + str(min_nb_pictures) + \" and \" + str(max_nb_pictures))\n",
    "        print(\"The average number of pictures per person is: \" + str(sum(pictures_nbs)/len(pictures_nbs)))\n",
    "        print(\" ------------------------------------------------------------------------\\n\")\n",
    "        \n",
    "        \n",
    "    # You must override __getitem__ and __len__\n",
    "    def __getitem__(self, index, visualization=False):\n",
    "        \"\"\" ---------------------------------------------------------------------------------------------\n",
    "            An item is made up of 3 images (P, P, N) and 2 target (1, 0) specifying that the 2 first \n",
    "            images are the same and the first and the third are different. The images are represented  \n",
    "            through TENSORS. \n",
    "            \n",
    "            If visualize = True: the image is printed \n",
    "        ----------------------------------------------------------------------------------------------- \"\"\" \n",
    "        if not DO_LEARN:\n",
    "            visualization=True\n",
    "            print(\"IN GET ITEM: the index in the dataset is: \" + str(index) + \"\\n\")\n",
    "        \n",
    "        if visualization: \n",
    "            with zipfile.ZipFile(ZIP_PATH, 'r') as archive:\n",
    "                for i, image_name in enumerate(self.train_not_formatted_data[index]):\n",
    "                    print(\"Face \" + str(i) + \": \")\n",
    "                    image = Image.open(BytesIO(archive.read(image_name))).convert(\"RGB\")       \n",
    "                    imgplot = plt.imshow(image)\n",
    "                    plt.show()\n",
    "        \n",
    "        return self.train_data[index], self.train_labels[index]\n",
    "\n",
    "      \n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Total number of samples in the dataset\n",
    "        \"\"\"\n",
    "        return len(self.train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEFINITION OF THE NEURAL NETWORK "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "   def __init__(self):\n",
    "      super().__init__()\n",
    "      \n",
    "      self.conv1 = nn.Conv2d(3, 64, 7)\n",
    "      self.pool1 = nn.MaxPool2d(2)\n",
    "      self.conv2 = nn.Conv2d(64, 128, 5)\n",
    "      self.conv3 = nn.Conv2d(128, 256, 5)\n",
    "      self.linear1 = nn.Linear(2304, 512)\n",
    "      \n",
    "      self.linear2 = nn.Linear(512, 2)\n",
    "      \n",
    "   def forward(self, data):\n",
    "      res = []\n",
    "      for i in range(2): # Siamese nets; sharing weights\n",
    "         x = data[i]\n",
    "         x = self.conv1(x)\n",
    "         x = F.relu(x)\n",
    "         x = self.pool1(x)\n",
    "         x = self.conv2(x)\n",
    "         x = F.relu(x)\n",
    "         x = self.conv3(x)\n",
    "         x = F.relu(x)\n",
    "         \n",
    "         x = x.view(x.shape[0], -1)\n",
    "         x = self.linear1(x)\n",
    "         res.append(F.relu(x))\n",
    "         \n",
    "      res = torch.abs(res[1] - res[0])\n",
    "      res = self.linear2(res)\n",
    "      return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEFINITION OF THE TRAINING FUNCTION "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, epoch, optimizer):\n",
    "\n",
    "    model.train()\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        for i in range(len(data)):\n",
    "            data[i] = data[i].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output_positive = model(data[:2])\n",
    "        output_negative = model(data[0:3:2])\n",
    "\n",
    "        target = target.type(torch.LongTensor).to(device)\n",
    "        target_positive = torch.squeeze(target[:,0])\n",
    "        target_negative = torch.squeeze(target[:,1])\n",
    "\n",
    "        loss_positive = F.cross_entropy(output_positive, target_positive)\n",
    "        loss_negative = F.cross_entropy(output_negative, target_negative)\n",
    "\n",
    "        loss = loss_positive + loss_negative\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % 10 == 0: # Print the state of the training each 10 batches (i.e each 10*size_batch considered examples)\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "            epoch, batch_idx*BATCH_SIZE, len(train_loader.dataset), 100. * batch_idx*BATCH_SIZE / len(train_loader.dataset),\n",
    "            loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEFINITION OF THE TESTING FUNCTION "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device, test_loader):\n",
    "   model.eval()\n",
    "   \n",
    "   with torch.no_grad():\n",
    "      accurate_labels = 0\n",
    "      all_labels = 0\n",
    "      loss = 0\n",
    "      for batch_idx, (data, target) in enumerate(test_loader):\n",
    "         for i in range(len(data)):\n",
    "            data[i] = data[i].to(device)\n",
    "            \n",
    "         output_positive = model(data[:2])\n",
    "         output_negative = model(data[0:3:2])\n",
    "            \n",
    "         target = target.type(torch.LongTensor).to(device)\n",
    "         target_positive = torch.squeeze(target[:,0])\n",
    "         target_negative = torch.squeeze(target[:,1])\n",
    "            \n",
    "         loss_positive = F.cross_entropy(output_positive, target_positive)\n",
    "         loss_negative = F.cross_entropy(output_negative, target_negative)\n",
    "            \n",
    "         loss = loss + loss_positive + loss_negative\n",
    "            \n",
    "         accurate_labels_positive = torch.sum(torch.argmax(output_positive, dim=1) == target_positive).cpu()\n",
    "         accurate_labels_negative = torch.sum(torch.argmax(output_negative, dim=1) == target_negative).cpu()\n",
    "            \n",
    "         accurate_labels = accurate_labels + accurate_labels_positive + accurate_labels_negative\n",
    "         all_labels = all_labels + len(target_positive) + len(target_negative)\n",
    "      \n",
    "      accuracy = 100. * accurate_labels / all_labels\n",
    "      print('Test accuracy: {}/{} ({:.3f}%)\\tLoss: {:.6f}'.format(accurate_labels, all_labels, accuracy, loss))\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEFINITION OF THE ONE-SHOT FUNCTION "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oneshot(model, device, data):\n",
    "   model.eval()\n",
    "\n",
    "   with torch.no_grad():\n",
    "      for i in range(len(data)):\n",
    "            data[i] = data[i].to(device)\n",
    "      \n",
    "      output = model(data)\n",
    "      return torch.squeeze(torch.argmax(output, dim=1)).cpu().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAIN FUNCTION "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################\n",
    "#       FUNCTION main                   #\n",
    "#########################################\n",
    "\n",
    "def main():\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  # Specifies where the torch.tensor is allocated\n",
    "    trans = transforms.Compose([transforms.CenterCrop(28), transforms.ToTensor(), transforms.Normalize((0.5,), (1.0,))]) # If applied, a dimensional error is raised \n",
    "    name_model = \"siamese_face\" + \"_\" + ZIP_PATH.split(\"datasets/\")[1].split(\".zip\")[0]\n",
    "    extension_model = \".pt\"\n",
    "\n",
    "    model = Net().to(device)\n",
    "    \n",
    "    ###############################################\n",
    "    # Definition of a training and a testing set \n",
    "    ############################################### \n",
    "\n",
    "    archive = zipfile.ZipFile(ZIP_PATH, 'r')\n",
    "    all_file_names = archive.namelist()\n",
    "    #shuffle(all_file_names)\n",
    "    train_files = all_file_names[:round(RATION_TRAIN_SET*len(all_file_names))]\n",
    "    test_files = all_file_names[round(RATION_TRAIN_SET*len(all_file_names)):]\n",
    "    \n",
    "    \n",
    "    if DO_LEARN:\n",
    "   \n",
    "        ##################\n",
    "        #  training mode\n",
    "        ##################\n",
    "        train_loader = torch.utils.data.DataLoader(Face_DS(file_names=train_files, transform=trans), batch_size=BATCH_SIZE, shuffle=True)\n",
    "        test_loader = torch.utils.data.DataLoader(Face_DS(file_names=test_files, transform=trans), batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "        optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "        for epoch in range(NUM_EPOCH):\n",
    "            train(model, device, train_loader, epoch, optimizer)\n",
    "            test(model, device, test_loader)\n",
    "\n",
    "        if SAVE_MODEL:\n",
    "            torch.save(model, (name_model + extension_model))# + '{:03}' .format(epoch))\n",
    "            print(\"Model is saved!\")\n",
    "      \n",
    "    else: # prediction\n",
    "        dataset = Face_DS(file_names=test_files, transform=trans)\n",
    "        prediction_loader = torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=True) # batch_size = Nb of pairs you want to test \n",
    "      \n",
    "        load_model_path = name_model + extension_model # os.getcwd() + \"/\" +  name_model + \"000\" + extension_model\n",
    "        model = torch.load(load_model_path)\n",
    "        \n",
    "        #####################################################################\n",
    "        # Data: list containing the tensor representations of the 2 images\n",
    "        #####################################################################\n",
    "        data = []\n",
    "        should_be_the_same = False \n",
    "        \n",
    "        if should_be_the_same:\n",
    "            print(\"GROUNDTRUE: The 2 faces are the same \")\n",
    "            data.extend(next(iter(prediction_loader))[0][:2])  # The 2 faces are the same \n",
    "        else: \n",
    "            print(\"GROUNDTRUE: The 2 faces are different \")\n",
    "            data.extend(next(iter(prediction_loader))[0][:3:2]) # The 2 faces are different \n",
    "\n",
    "        #print(\"The data given to the onshot function is: \" + str(data))\n",
    "\n",
    "        same = oneshot(model, device, data)\n",
    "        if same > 0:\n",
    "            print('=> PREDICTION: These two images represent the same person')\n",
    "        else:\n",
    "            print(\"=> PREDICTION: These two images don't represent the same person\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ---------------- Report about the quantity of data  -------------------- \n",
      "The number of different people in set is: 132\n",
      "The number of pictures per person is between: 1 and 18\n",
      "The average number of pictures per person is: 8.159090909090908\n",
      " ------------------------------------------------------------------------\n",
      "\n",
      " ---------------- Report about the quantity of data  -------------------- \n",
      "The number of different people in set is: 29\n",
      "The number of pictures per person is between: 1 and 18\n",
      "The average number of pictures per person is: 12.413793103448276\n",
      " ------------------------------------------------------------------------\n",
      "\n",
      "Train Epoch: 0 [0/1036 (0%)]\tLoss: 1.386440\n",
      "Train Epoch: 0 [160/1036 (15%)]\tLoss: 1.397022\n",
      "Train Epoch: 0 [320/1036 (31%)]\tLoss: 1.345428\n",
      "Train Epoch: 0 [480/1036 (46%)]\tLoss: 1.401585\n",
      "Train Epoch: 0 [640/1036 (62%)]\tLoss: 1.257375\n",
      "Train Epoch: 0 [800/1036 (77%)]\tLoss: 1.330747\n",
      "Train Epoch: 0 [960/1036 (93%)]\tLoss: 1.172502\n",
      "Test accuracy: 434/712 (60.000%)\tLoss: 29.090963\n",
      "Train Epoch: 1 [0/1036 (0%)]\tLoss: 1.310895\n",
      "Train Epoch: 1 [160/1036 (15%)]\tLoss: 1.223237\n",
      "Train Epoch: 1 [320/1036 (31%)]\tLoss: 1.330809\n",
      "Train Epoch: 1 [480/1036 (46%)]\tLoss: 1.375478\n",
      "Train Epoch: 1 [640/1036 (62%)]\tLoss: 1.389528\n",
      "Train Epoch: 1 [800/1036 (77%)]\tLoss: 1.317064\n",
      "Train Epoch: 1 [960/1036 (93%)]\tLoss: 1.353360\n",
      "Test accuracy: 496/712 (69.000%)\tLoss: 29.923391\n",
      "Model is saved!\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "   main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

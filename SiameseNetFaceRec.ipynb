{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GLOBAL VARIABLES "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "FILE = \"Aberdeen\"#.zip\"\n",
    "PATH = os.path.join(os.getcwd(),\"datasets\", FILE)\n",
    "BATCH_SIZE = 16\n",
    "DO_LEARN = True\n",
    "LEARNING_RATE = 0.001\n",
    "NUM_EPOCH = 10 \n",
    "WEIGHT_DECAY = 0.0001\n",
    "SAVE_MODEL=True "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MANAGEMENT OF THE DATA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import glob\n",
    "import random \n",
    "\n",
    "import os.path as osp\n",
    "from PIL import Image\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.utils.data\n",
    "\n",
    "class Face_DS(torch.utils.data.Dataset):\n",
    "    \n",
    "    \"\"\"\n",
    "    A customized data loader.\n",
    "    \"\"\"\n",
    "    def __init__(self, root=PATH, train=False, transform=None):\n",
    "        \"\"\" Intialize the dataset\n",
    "        \"\"\"\n",
    "        \n",
    "        self.filenames = []\n",
    "        self.root = root\n",
    "        self.train = train \n",
    "\n",
    "        if transform is None: \n",
    "            self.transform = transforms.ToTensor()\n",
    "        else: \n",
    "            self.transform=transform\n",
    "        \n",
    "        #archive = zipfile.ZipFile(path, 'r')^^\n",
    "        #filenames = [name for name in archive.namelist() if name.endswith('.jpg')]\n",
    "        #print(\"The path is \" + path)\n",
    "        filenames = glob.glob(osp.join(self.root, '*.jpg'))\n",
    "        #print(\"The filenames are: \" + str(filenames))\n",
    "        \n",
    "        for fn in filenames:\n",
    "            self.filenames.append(fn)\n",
    "        self.len = len(self.filenames)\n",
    "        \n",
    "        self.training_data = self.filenames\n",
    "        self.training_labels = list(range(0, self.len))\n",
    "\n",
    "\n",
    "        \n",
    "    # You must override __getitem__ and __len__\n",
    "    def __getitem__(self, index, visualization=False):\n",
    "        \"\"\" ---------------------------------------------------------------------------------------------\n",
    "            An item is made up of 3 images (P, P, N) and 2 target (1, 0) specifying that the 2 first \n",
    "            images are the same and the first and the third are different \n",
    "            \n",
    "            If visualize = True: the image is printed \n",
    "            BUG: if visualization = True: RunTimeError due to the returned content (??)\n",
    "        ----------------------------------------------------------------------------------------------- \"\"\" \n",
    "\n",
    "        # Visualization Part \n",
    "        if visualization: \n",
    "            # Use the torch dataloader to iterate through the dataset\n",
    "            loader = torch.utils.data.DataLoader(self, shuffle=False, num_workers=0) #, batch_size=3)  \n",
    "    \n",
    "            #get the image\n",
    "            for i_batch, sample_batched in enumerate(loader):\n",
    "                if i_batch == index:\n",
    "                    image = sample_batched\n",
    "                    break\n",
    "            \n",
    "            # show images\n",
    "            print(\"Here is the face: \")\n",
    "            plt.figure(figsize=(16,8))\n",
    "            img = torchvision.utils.make_grid(image)\n",
    "            npimg = img.numpy()\n",
    "            plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    " \n",
    "        \n",
    "        imageData = Image.open(self.training_data[index]).convert(\"RGB\") #[0]]) # Single element: later: should be a list\n",
    "        target = torch.from_numpy(np.array(self.training_labels[index]) )\n",
    "\n",
    "        #print(\"The tensor representing the imageData is : \" + str(self.transform(imageData)))\n",
    "        #print(\"The tensor representing the target is : \" + str(target.float()))\n",
    "\n",
    "        return self.transform(imageData), target.float() # Return x (i.e. image through a tensor) and y (i.e. label of the image)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Total number of samples in the dataset\n",
    "        \"\"\"\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          ...,\n",
       "          [0.3529, 0.5059, 0.7176,  ..., 0.7020, 0.7451, 0.6588],\n",
       "          [0.2706, 0.3922, 0.6157,  ..., 0.5725, 0.6275, 0.5765],\n",
       "          [0.1176, 0.2627, 0.5216,  ..., 0.4941, 0.5608, 0.5686]],\n",
       " \n",
       "         [[0.9961, 0.9961, 0.9961,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [0.9961, 0.9961, 0.9961,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          ...,\n",
       "          [0.3294, 0.4824, 0.6980,  ..., 0.6745, 0.7216, 0.6392],\n",
       "          [0.2392, 0.3647, 0.5922,  ..., 0.5451, 0.6039, 0.5529],\n",
       "          [0.0784, 0.2235, 0.4863,  ..., 0.4667, 0.5373, 0.5451]],\n",
       " \n",
       "         [[0.9804, 0.9804, 0.9804,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [0.9765, 0.9765, 0.9765,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [0.9608, 0.9608, 0.9608,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          ...,\n",
       "          [0.3765, 0.5216, 0.7216,  ..., 0.7373, 0.7686, 0.6667],\n",
       "          [0.2824, 0.3961, 0.6078,  ..., 0.6157, 0.6510, 0.5922],\n",
       "          [0.1137, 0.2549, 0.4980,  ..., 0.5373, 0.5922, 0.5843]]]),\n",
       " tensor(1.))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "########################\n",
    "#         TEST         #\n",
    "########################\n",
    "\n",
    "ds = Face_DS()\n",
    "ds.__getitem__(1, visualization=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEFINITION OF THE NEURAL NETWORK "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch import optim\n",
    "\n",
    "class Net(nn.Module):\n",
    "   def __init__(self):\n",
    "      super().__init__()\n",
    "      \n",
    "      self.conv1 = nn.Conv2d(1, 64, 7)\n",
    "      self.pool1 = nn.MaxPool2d(2)\n",
    "      self.conv2 = nn.Conv2d(64, 128, 5)\n",
    "      self.conv3 = nn.Conv2d(128, 256, 5)\n",
    "      self.linear1 = nn.Linear(2304, 512)\n",
    "      \n",
    "      self.linear2 = nn.Linear(512, 2)\n",
    "      \n",
    "   def forward(self, data):\n",
    "      res = []\n",
    "      for i in range(2): # Siamese nets; sharing weights\n",
    "         x = data[i]\n",
    "         x = self.conv1(x)\n",
    "         x = F.relu(x)\n",
    "         x = self.pool1(x)\n",
    "         x = self.conv2(x)\n",
    "         x = F.relu(x)\n",
    "         x = self.conv3(x)\n",
    "         x = F.relu(x)\n",
    "         \n",
    "         x = x.view(x.shape[0], -1)\n",
    "         x = self.linear1(x)\n",
    "         res.append(F.relu(x))\n",
    "         \n",
    "      res = torch.abs(res[1] - res[0])\n",
    "      res = self.linear2(res)\n",
    "      return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEFINITION OF THE TRAINING FUNCTION "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, epoch, optimizer):\n",
    "   model.train() # Just set the module in training mode\n",
    "   \n",
    "   for batch_idx, (data, target) in enumerate(train_loader):\n",
    "      for i in range(len(data)):\n",
    "         data[i] = data[i].to(device)\n",
    "         \n",
    "      optimizer.zero_grad()\n",
    "      output_positive = model(data[:2])\n",
    "      output_negative = model(data[0:3:2])\n",
    "      \n",
    "      target = target.type(torch.LongTensor).to(device)\n",
    "      target_positive = torch.squeeze(target[:,0])\n",
    "      target_negative = torch.squeeze(target[:,1])\n",
    "      \n",
    "      loss_positive = F.cross_entropy(output_positive, target_positive)\n",
    "      loss_negative = F.cross_entropy(output_negative, target_negative)\n",
    "      \n",
    "      loss = loss_positive + loss_negative\n",
    "      loss.backward()\n",
    "      \n",
    "      optimizer.step()\n",
    "      if batch_idx % 10 == 0:\n",
    "         print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "            epoch, batch_idx*batch_size, len(train_loader.dataset), 100. * batch_idx*batch_size / len(train_loader.dataset),\n",
    "            loss.item()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEFINITION OF THE TESTING FUNCTION "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device, test_loader):\n",
    "   model.eval()\n",
    "   \n",
    "   with torch.no_grad():\n",
    "      accurate_labels = 0\n",
    "      all_labels = 0\n",
    "      loss = 0\n",
    "      for batch_idx, (data, target) in enumerate(test_loader):\n",
    "         for i in range(len(data)):\n",
    "            data[i] = data[i].to(device)\n",
    "            \n",
    "         output_positive = model(data[:2])\n",
    "         output_negative = model(data[0:3:2])\n",
    "            \n",
    "         target = target.type(torch.LongTensor).to(device)\n",
    "         target_positive = torch.squeeze(target[:,0])\n",
    "         target_negative = torch.squeeze(target[:,1])\n",
    "            \n",
    "         loss_positive = F.cross_entropy(output_positive, target_positive)\n",
    "         loss_negative = F.cross_entropy(output_negative, target_negative)\n",
    "            \n",
    "         loss = loss + loss_positive + loss_negative\n",
    "            \n",
    "         accurate_labels_positive = torch.sum(torch.argmax(output_positive, dim=1) == target_positive).cpu()\n",
    "         accurate_labels_negative = torch.sum(torch.argmax(output_negative, dim=1) == target_negative).cpu()\n",
    "            \n",
    "         accurate_labels = accurate_labels + accurate_labels_positive + accurate_labels_negative\n",
    "         all_labels = all_labels + len(target_positive) + len(target_negative)\n",
    "      \n",
    "      accuracy = 100. * accurate_labels / all_labels\n",
    "      print('Test accuracy: {}/{} ({:.3f}%)\\tLoss: {:.6f}'.format(accurate_labels, all_labels, accuracy, loss))\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEFINITION OF THE ONE-SHOT FUNCTION "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oneshot(model, device, data):\n",
    "   model.eval()\n",
    "\n",
    "   with torch.no_grad():\n",
    "      for i in range(len(data)):\n",
    "            data[i] = data[i].to(device)\n",
    "      \n",
    "      output = model(data)\n",
    "      return torch.squeeze(torch.argmax(output, dim=1)).cpu().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAIN FUNCTION "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  # Specifies where the torch.tensor is allocated\n",
    "trans = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (1.0,))])\n",
    "name_model = \"siamese_\"\n",
    "extension_model = \".pt\"\n",
    "   \n",
    "model = Net().to(device)\n",
    "   \n",
    "##################\n",
    "#  training mode\n",
    "##################\n",
    "train_loader = torch.utils.data.DataLoader(Face_DS(train=True, transform=trans), batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(Face_DS(train=False, transform=trans), batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "invalid argument 0: Sizes of tensors must match except in dimension 0. Got 528 and 496 in dimension 2 at /Users/soumith/code/builder/wheel/pytorch-src/aten/src/TH/generic/THTensorMath.cpp:3616",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-b93c38500056>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    312\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0mtransposed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0mtransposed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new_shared\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0melem_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__module__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'numpy'\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0melem_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'str_'\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0melem_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'string_'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: invalid argument 0: Sizes of tensors must match except in dimension 0. Got 528 and 496 in dimension 2 at /Users/soumith/code/builder/wheel/pytorch-src/aten/src/TH/generic/THTensorMath.cpp:3616"
     ]
    }
   ],
   "source": [
    "\n",
    "for epoch in range(NUM_EPOCH):\n",
    "            \n",
    "    #train(model, device, train_loader, epoch, optimizer)\n",
    "    model.train()\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        for i in range(len(data)):\n",
    "            data[i] = data[i].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output_positive = model(data[:2])\n",
    "        output_negative = model(data[0:3:2])\n",
    "      \n",
    "        target = target.type(torch.LongTensor).to(device)\n",
    "        target_positive = torch.squeeze(target[:,0])\n",
    "        target_negative = torch.squeeze(target[:,1])\n",
    "\n",
    "        loss_positive = F.cross_entropy(output_positive, target_positive)\n",
    "        loss_negative = F.cross_entropy(output_negative, target_negative)\n",
    "      \n",
    "        loss = loss_positive + loss_negative\n",
    "        loss.backward()\n",
    "      \n",
    "        optimizer.step()\n",
    "        if batch_idx % 10 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "            epoch, batch_idx*batch_size, len(train_loader.dataset), 100. * batch_idx*batch_size / len(train_loader.dataset),\n",
    "            loss.item()))\n",
    "            \n",
    "\n",
    "    test(model, device, test_loader)\n",
    "    \n",
    "if SAVE_MODEL:\n",
    "    torch.save(model, (name_model + '{:03}' + extension_model).format(epoch))\n",
    "    print(\"Model is saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OBSERVATION\n",
    "# nn.linear(512) Got 528 and 496 (+ 32 of difference) \n",
    "# nn.linear(496) Got 512 and 528"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GLOBAL VARIABLES and IMPORTS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import zipfile\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "import random\n",
    "from random import shuffle\n",
    "from string import digits\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os.path as osp\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "%matplotlib inline\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.utils.data\n",
    "\n",
    "ZIP_PATH = 'OneShotLearning_FR/datasets/1AberdeenCrop.zip' #aber&GTdb_crop.zip'\n",
    "\n",
    "EXTENSION = \".jpg\"\n",
    "SEPARATOR = \"_\" # !!! Format of the dabase: name[!]_id !!!\n",
    "RATION_TRAIN_SET = 0.75\n",
    "WITH_PROFILE = False # True if both frontally and in profile people \n",
    "SAVE_MODEL = True \n",
    "DO_LEARN = True \n",
    "DIFF_FACES = True # If true, we have different faces in the training and the testing set \n",
    "\n",
    "BATCH_SIZE = 16\n",
    "LEARNING_RATE = 0.001\n",
    "NUM_EPOCH = 50 \n",
    "WEIGHT_DECAY = 0.0001\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_and_test_sets(): \n",
    "    \n",
    "    all_labels = []\n",
    "    with zipfile.ZipFile(ZIP_PATH, 'r') as archive:\n",
    "        \n",
    "        file_names = archive.namelist()\n",
    "        train_filenames = []\n",
    "        test_filenames = []\n",
    "        \n",
    "        ############################################################\n",
    "        # CASE 1: same people in the training and the testing set\n",
    "        ############################################################\n",
    "        if not DIFF_FACES:\n",
    "            shuffle(file_names)\n",
    "            train_filenames = file_names[:round(RATION_TRAIN_SET*len(file_names))]\n",
    "            test_filenames = file_names[round(RATION_TRAIN_SET*len(file_names)):]\n",
    "        \n",
    "        ##############################################################\n",
    "        # CASE 2: different people in the training and the testing set\n",
    "        ##############################################################\n",
    "        else:\n",
    "            # Extract all the labels \n",
    "            for fn in file_names: # fn = name[!]_id\n",
    "                if fn == \".DS_Store\":\n",
    "                    continue\n",
    "                label = fn.split(SEPARATOR)[0]\n",
    "\n",
    "                try:\n",
    "                    all_labels.index(label)\n",
    "                except ValueError: \n",
    "                    all_labels.append(label)\n",
    "\n",
    "            # Build a training and a testing set with different labels\n",
    "            shuffle(all_labels)\n",
    "\n",
    "            train_labels = all_labels[:round(RATION_TRAIN_SET*len(all_labels))]\n",
    "            test_labels = all_labels[round(RATION_TRAIN_SET*len(all_labels)):]\n",
    "\n",
    "            for fn in file_names: # fn = name[!]_id\n",
    "                if fn == \".DS_Store\":\n",
    "                    continue\n",
    "\n",
    "                label = fn.split(SEPARATOR)[0]\n",
    "                try:\n",
    "                    train_labels.index(label)\n",
    "                    train_filenames.append(fn)\n",
    "                except ValueError: \n",
    "                    test_filenames.append(fn)\n",
    "\n",
    "    return train_filenames, test_filenames\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MANAGEMENT OF THE DATA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaceImage():\n",
    "    def __init__(self, path, trans_image):\n",
    "        self.path = path\n",
    "        self.trans_img = trans_image\n",
    "    \n",
    "    def isIqual(self, other_image):\n",
    "        return other_image.path == self.path "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "STILL TODO: \n",
    "    - Putting more images in the database \n",
    "\"\"\"\n",
    "class Face_DS(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, file_names=None, transform=None):\n",
    "        \n",
    "        self.transform = transforms.ToTensor() if transform is None else transform\n",
    "\n",
    "        faces_dic = {}\n",
    "        \n",
    "        with zipfile.ZipFile(ZIP_PATH, 'r') as archive:\n",
    "            \n",
    "            if file_names is None:\n",
    "                file_names = archive.namelist()\n",
    "            \n",
    "            #################################\n",
    "            # Order the picture per label \n",
    "            #################################\n",
    "            for fn in file_names: # fn = name[!]_id\n",
    "                if fn == \".DS_Store\":\n",
    "                    continue\n",
    "                    \n",
    "                label = fn.split(SEPARATOR)[0]\n",
    "                \n",
    "                ###### Check if the picture represents the person in profile ###### \n",
    "                is_profile = True if label[-1] == \"!\" else False\n",
    "\n",
    "                if not WITH_PROFILE and is_profile: \n",
    "                    next # Case where the person is in profile while we don't want this type of picture\n",
    "                \n",
    "                if WITH_PROFILE and is_profile:\n",
    "                    label = label[:-1]\n",
    "\n",
    "                formatted_image = self.transform(Image.open(BytesIO(archive.read(fn))).convert(\"RGB\"))\n",
    "                img = FaceImage(fn, formatted_image)\n",
    "\n",
    "                try: \n",
    "                    faces_dic[label].append(img)\n",
    "                except KeyError: \n",
    "                    faces_dic[label] = [img]\n",
    "                    \n",
    "        all_labels = list(faces_dic.keys())\n",
    "        nb_labels = len(all_labels)\n",
    "        \n",
    "        ########################################################################\n",
    "        # Build triplet supporting the dataset (ensures balanced classes)\n",
    "        ########################################################################\n",
    "        self.train_data = []\n",
    "        self.train_labels = []\n",
    "        self.train_not_formatted_data = []\n",
    "        \n",
    "        ################ Consider each person ##########################\n",
    "        for label, pictures_list in faces_dic.items():\n",
    "            pic_ind_pos = list(range(len(pictures_list)))\n",
    "            shuffle(pic_ind_pos)\n",
    "            labels_indexes_neg = [x for x in range(0, nb_labels) if x != all_labels.index(label)]\n",
    "\n",
    "            \n",
    "            ################ Consider each picture of the person #####################\n",
    "            for i, picture_ref in enumerate(pictures_list):\n",
    "                \n",
    "                try: \n",
    "                    if not picture_ref.isIqual(pictures_list[pic_ind_pos[-1]]):\n",
    "                        picture_positive = pictures_list[pic_ind_pos.pop()]\n",
    "                    else:\n",
    "                        picture_positive = pictures_list[pic_ind_pos.pop(-2)]\n",
    "                except IndexError: \n",
    "                    # !! Means that the current label has no remaining other picture !! \n",
    "                    break\n",
    "                \n",
    "                # Pick a random different person \n",
    "                label_neg = all_labels[random.choice(labels_indexes_neg)]\n",
    "                picture_negative = random.choice(faces_dic[label_neg]) \n",
    "                \n",
    "                self.train_not_formatted_data.append([picture_ref.path, picture_positive.path, picture_negative.path])\n",
    "\n",
    "                self.train_data.append([picture_ref.trans_img , picture_positive.trans_img , picture_negative.trans_img ]) # torch.stack is not applied because we want a list of tensors \n",
    "                self.train_labels.append([1,0])\n",
    "                \n",
    "        #self.train_data = torch.stack(self.train_data)\n",
    "        self.train_labels = torch.tensor(self.train_labels)\n",
    "        \n",
    "        #############################################\n",
    "        # Report about the quantity of data \n",
    "        #############################################\n",
    "        pictures_nbs = [len(pictures_list) for person_name, pictures_list in faces_dic.items()]\n",
    "        max_nb_pictures = max(pictures_nbs)\n",
    "        min_nb_pictures = 1 #min(pictures_nbs) # !!!! TOSOLVe: strange error: TypeError: 'list' object is not callable, while max works\n",
    "\n",
    "        print(\" ---------------- Report about the quantity of data  -------------------- \")\n",
    "        print(\"The total quantity of pairs used as data is: \" + str(2*len(self.train_labels)))\n",
    "        print(\"The number of different people in set is: \" + str(nb_labels))\n",
    "        print(\"The number of pictures per person is between: \" + str(min_nb_pictures) + \" and \" + str(max_nb_pictures))\n",
    "        print(\"The average number of pictures per person is: \" + str(sum(pictures_nbs)/len(pictures_nbs)))\n",
    "        print(\" ------------------------------------------------------------------------\\n\")\n",
    "        \n",
    "        \n",
    "    # You must override __getitem__ and __len__\n",
    "    def __getitem__(self, index, visualization=False):\n",
    "        \"\"\" ---------------------------------------------------------------------------------------------\n",
    "            An item is made up of 3 images (P, P, N) and 2 target (1, 0) specifying that the 2 first \n",
    "            images are the same and the first and the third are different. The images are represented  \n",
    "            through TENSORS. \n",
    "            \n",
    "            If visualize = True: the image is printed \n",
    "        ----------------------------------------------------------------------------------------------- \"\"\" \n",
    "        if not DO_LEARN:\n",
    "            visualization=True\n",
    "            print(\"IN GET ITEM: the index in the dataset is: \" + str(index) + \"\\n\")\n",
    "        \n",
    "        if visualization: \n",
    "            with zipfile.ZipFile(ZIP_PATH, 'r') as archive:\n",
    "                for i, image_name in enumerate(self.train_not_formatted_data[index]):\n",
    "                    print(\"Face \" + str(i) + \": \")\n",
    "                    image = Image.open(BytesIO(archive.read(image_name))).convert(\"RGB\")       \n",
    "                    imgplot = plt.imshow(image)\n",
    "                    plt.show()\n",
    "        \n",
    "        return self.train_data[index], self.train_labels[index]\n",
    "\n",
    "      \n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Total number of samples in the dataset\n",
    "        \"\"\"\n",
    "        return len(self.train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEFINITION OF THE NEURAL NETWORK "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "   def __init__(self):\n",
    "      super().__init__()\n",
    "      \n",
    "      self.conv1 = nn.Conv2d(3, 64, 7)\n",
    "      self.pool1 = nn.MaxPool2d(2)\n",
    "      self.conv2 = nn.Conv2d(64, 128, 5)\n",
    "      self.conv3 = nn.Conv2d(128, 256, 5)\n",
    "      self.linear1 = nn.Linear(2304, 512)\n",
    "      \n",
    "      self.linear2 = nn.Linear(512, 2)\n",
    "      \n",
    "   def forward(self, data):\n",
    "      res = []\n",
    "      for i in range(2): # Siamese nets; sharing weights\n",
    "         x = data[i]\n",
    "         x = self.conv1(x)\n",
    "         x = F.relu(x)\n",
    "         x = self.pool1(x)\n",
    "         x = self.conv2(x)\n",
    "         x = F.relu(x)\n",
    "         x = self.conv3(x)\n",
    "         x = F.relu(x)\n",
    "         \n",
    "         x = x.view(x.shape[0], -1)\n",
    "         x = self.linear1(x)\n",
    "         res.append(F.relu(x))\n",
    "         \n",
    "      res = torch.abs(res[1] - res[0])\n",
    "      res = self.linear2(res)\n",
    "      return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEFINITION OF THE TRAINING FUNCTION "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, epoch, optimizer):\n",
    "\n",
    "    model.train()\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        for i in range(len(data)):\n",
    "            data[i] = data[i].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output_positive = model(data[:2])\n",
    "        output_negative = model(data[0:3:2])\n",
    "\n",
    "        target = target.type(torch.LongTensor).to(device)\n",
    "        target_positive = torch.squeeze(target[:,0])\n",
    "        target_negative = torch.squeeze(target[:,1])\n",
    "\n",
    "        loss_positive = F.cross_entropy(output_positive, target_positive)\n",
    "        loss_negative = F.cross_entropy(output_negative, target_negative)\n",
    "\n",
    "        loss = loss_positive + loss_negative\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % 10 == 0: # Print the state of the training each 10 batches (i.e each 10*size_batch considered examples)\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "            epoch, batch_idx*BATCH_SIZE, len(train_loader.dataset), 100. * batch_idx*BATCH_SIZE / len(train_loader.dataset),\n",
    "            loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEFINITION OF THE TESTING FUNCTION "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device, test_loader):\n",
    "   model.eval()\n",
    "   \n",
    "   with torch.no_grad():\n",
    "      accurate_labels = 0\n",
    "      all_labels = 0\n",
    "      loss = 0\n",
    "      for batch_idx, (data, target) in enumerate(test_loader):\n",
    "         for i in range(len(data)):\n",
    "            data[i] = data[i].to(device)\n",
    "            \n",
    "         output_positive = model(data[:2])\n",
    "         output_negative = model(data[0:3:2])\n",
    "            \n",
    "         target = target.type(torch.LongTensor).to(device)\n",
    "         target_positive = torch.squeeze(target[:,0])\n",
    "         target_negative = torch.squeeze(target[:,1])\n",
    "            \n",
    "         loss_positive = F.cross_entropy(output_positive, target_positive)\n",
    "         loss_negative = F.cross_entropy(output_negative, target_negative)\n",
    "            \n",
    "         loss = loss + loss_positive + loss_negative\n",
    "            \n",
    "         accurate_labels_positive = torch.sum(torch.argmax(output_positive, dim=1) == target_positive).cpu()\n",
    "         accurate_labels_negative = torch.sum(torch.argmax(output_negative, dim=1) == target_negative).cpu()\n",
    "            \n",
    "         accurate_labels = accurate_labels + accurate_labels_positive + accurate_labels_negative\n",
    "         all_labels = all_labels + len(target_positive) + len(target_negative)\n",
    "      \n",
    "      accuracy = 100. * accurate_labels / all_labels\n",
    "      print('Test accuracy: {}/{} ({:.3f}%)\\tLoss: {:.6f}'.format(accurate_labels, all_labels, accuracy, loss))\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEFINITION OF THE ONE-SHOT FUNCTION "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oneshot(model, device, data):\n",
    "   model.eval()\n",
    "\n",
    "   with torch.no_grad():\n",
    "      for i in range(len(data)):\n",
    "            data[i] = data[i].to(device)\n",
    "      \n",
    "      output = model(data)\n",
    "      return torch.squeeze(torch.argmax(output, dim=1)).cpu().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAIN FUNCTION "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################\n",
    "#       FUNCTION main                   #\n",
    "#########################################\n",
    "\n",
    "def main():\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  # Specifies where the torch.tensor is allocated\n",
    "    trans = transforms.Compose([transforms.CenterCrop(28), transforms.ToTensor(), transforms.Normalize((0.5,), (1.0,))]) # If applied, a dimensional error is raised \n",
    "    name_model = \"siameseFace\" + \"_\" + ZIP_PATH.split(\"datasets/\")[1].split(\".zip\")[0] + (\"_diff\" if DIFF_FACES else \"_same\")\n",
    "    extension_model = \".pt\"\n",
    "\n",
    "    model = Net().to(device)\n",
    "    \n",
    "    ###############################################\n",
    "    # Definition of a training and a testing set \n",
    "    ############################################### \n",
    "    train_files, test_files = get_train_and_test_sets()\n",
    "\n",
    "    if DO_LEARN:\n",
    "   \n",
    "        ##################\n",
    "        #  training mode\n",
    "        ##################\n",
    "        train_loader = torch.utils.data.DataLoader(Face_DS(file_names=train_files, transform=trans), batch_size=BATCH_SIZE, shuffle=True)\n",
    "        test_loader = torch.utils.data.DataLoader(Face_DS(file_names=test_files, transform=trans), batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "        optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "        for epoch in range(NUM_EPOCH):\n",
    "            train(model, device, train_loader, epoch, optimizer)\n",
    "            test(model, device, test_loader)\n",
    "\n",
    "        if SAVE_MODEL:\n",
    "            torch.save(model, (name_model + extension_model))# + '{:03}' .format(epoch))\n",
    "            print(\"Model is saved!\")\n",
    "      \n",
    "    else: # prediction\n",
    "        dataset = Face_DS(file_names=test_files, transform=trans)\n",
    "        prediction_loader = torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=True) # batch_size = Nb of pairs you want to test \n",
    "      \n",
    "        load_model_path = name_model + extension_model # os.getcwd() + \"/\" +  name_model + \"000\" + extension_model\n",
    "        model = torch.load(load_model_path)\n",
    "        \n",
    "        #####################################################################\n",
    "        # Data: list containing the tensor representations of the 2 images\n",
    "        #####################################################################\n",
    "        data = []\n",
    "        should_be_the_same = True \n",
    "        \n",
    "        if should_be_the_same:\n",
    "            data.extend(next(iter(prediction_loader))[0][:2])  # The 2 faces are the same \n",
    "            print(\"GROUNDTRUE: The 2 faces are the same \")\n",
    "\n",
    "        else: \n",
    "            data.extend(next(iter(prediction_loader))[0][:3:2]) # The 2 faces are different \n",
    "            print(\"GROUNDTRUE: The 2 faces are different \")\n",
    "\n",
    "\n",
    "        #print(\"The data given to the onshot function is: \" + str(data))\n",
    "\n",
    "        same = oneshot(model, device, data)\n",
    "        if same > 0:\n",
    "            print('=> PREDICTION: These two images represent the same person')\n",
    "        else:\n",
    "            print(\"=> PREDICTION: These two images don't represent the same person\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ---------------- Report about the quantity of data  -------------------- \n",
      "The total quantity of pairs used as data is: 818\n",
      "The number of different people in set is: 82\n",
      "The number of pictures per person is between: 1 and 18\n",
      "The average number of pictures per person is: 5.426829268292683\n",
      " ------------------------------------------------------------------------\n",
      "\n",
      " ---------------- Report about the quantity of data  -------------------- \n",
      "The total quantity of pairs used as data is: 466\n",
      "The number of different people in set is: 28\n",
      "The number of pictures per person is between: 1 and 18\n",
      "The average number of pictures per person is: 8.642857142857142\n",
      " ------------------------------------------------------------------------\n",
      "\n",
      "Train Epoch: 0 [0/409 (0%)]\tLoss: 1.386506\n",
      "Train Epoch: 0 [160/409 (39%)]\tLoss: 1.374329\n",
      "Train Epoch: 0 [320/409 (78%)]\tLoss: 1.380005\n",
      "Test accuracy: 233/466 (50.000%)\tLoss: 20.626595\n",
      "Train Epoch: 1 [0/409 (0%)]\tLoss: 1.337603\n",
      "Train Epoch: 1 [160/409 (39%)]\tLoss: 1.367141\n",
      "Train Epoch: 1 [320/409 (78%)]\tLoss: 1.433466\n",
      "Test accuracy: 250/466 (53.000%)\tLoss: 20.543034\n",
      "Train Epoch: 2 [0/409 (0%)]\tLoss: 1.344169\n",
      "Train Epoch: 2 [160/409 (39%)]\tLoss: 1.287729\n",
      "Train Epoch: 2 [320/409 (78%)]\tLoss: 1.325969\n",
      "Test accuracy: 248/466 (53.000%)\tLoss: 20.569370\n",
      "Train Epoch: 3 [0/409 (0%)]\tLoss: 1.473789\n",
      "Train Epoch: 3 [160/409 (39%)]\tLoss: 1.393198\n",
      "Train Epoch: 3 [320/409 (78%)]\tLoss: 1.243491\n",
      "Test accuracy: 253/466 (54.000%)\tLoss: 20.650414\n",
      "Train Epoch: 4 [0/409 (0%)]\tLoss: 1.366533\n",
      "Train Epoch: 4 [160/409 (39%)]\tLoss: 1.367719\n",
      "Train Epoch: 4 [320/409 (78%)]\tLoss: 1.307022\n",
      "Test accuracy: 256/466 (54.000%)\tLoss: 20.788162\n",
      "Train Epoch: 5 [0/409 (0%)]\tLoss: 1.513744\n",
      "Train Epoch: 5 [160/409 (39%)]\tLoss: 1.419301\n",
      "Train Epoch: 5 [320/409 (78%)]\tLoss: 1.298156\n",
      "Test accuracy: 267/466 (57.000%)\tLoss: 20.309929\n",
      "Train Epoch: 6 [0/409 (0%)]\tLoss: 1.407325\n",
      "Train Epoch: 6 [160/409 (39%)]\tLoss: 1.248719\n",
      "Train Epoch: 6 [320/409 (78%)]\tLoss: 1.387997\n",
      "Test accuracy: 274/466 (58.000%)\tLoss: 20.289557\n",
      "Train Epoch: 7 [0/409 (0%)]\tLoss: 1.315703\n",
      "Train Epoch: 7 [160/409 (39%)]\tLoss: 1.295404\n",
      "Train Epoch: 7 [320/409 (78%)]\tLoss: 1.225974\n",
      "Test accuracy: 262/466 (56.000%)\tLoss: 20.402174\n",
      "Train Epoch: 8 [0/409 (0%)]\tLoss: 1.354505\n",
      "Train Epoch: 8 [160/409 (39%)]\tLoss: 1.246289\n",
      "Train Epoch: 8 [320/409 (78%)]\tLoss: 1.351550\n",
      "Test accuracy: 268/466 (57.000%)\tLoss: 20.322586\n",
      "Train Epoch: 9 [0/409 (0%)]\tLoss: 1.174379\n",
      "Train Epoch: 9 [160/409 (39%)]\tLoss: 1.370442\n",
      "Train Epoch: 9 [320/409 (78%)]\tLoss: 1.311719\n",
      "Test accuracy: 264/466 (56.000%)\tLoss: 20.700029\n",
      "Train Epoch: 10 [0/409 (0%)]\tLoss: 1.247149\n",
      "Train Epoch: 10 [160/409 (39%)]\tLoss: 1.318680\n",
      "Train Epoch: 10 [320/409 (78%)]\tLoss: 1.368961\n",
      "Test accuracy: 270/466 (57.000%)\tLoss: 21.193903\n",
      "Train Epoch: 11 [0/409 (0%)]\tLoss: 1.191802\n",
      "Train Epoch: 11 [160/409 (39%)]\tLoss: 1.277035\n",
      "Train Epoch: 11 [320/409 (78%)]\tLoss: 1.255794\n",
      "Test accuracy: 261/466 (56.000%)\tLoss: 20.976248\n",
      "Train Epoch: 12 [0/409 (0%)]\tLoss: 1.226779\n",
      "Train Epoch: 12 [160/409 (39%)]\tLoss: 1.341549\n",
      "Train Epoch: 12 [320/409 (78%)]\tLoss: 1.348816\n",
      "Test accuracy: 268/466 (57.000%)\tLoss: 20.565243\n",
      "Train Epoch: 13 [0/409 (0%)]\tLoss: 1.306084\n",
      "Train Epoch: 13 [160/409 (39%)]\tLoss: 1.094409\n",
      "Train Epoch: 13 [320/409 (78%)]\tLoss: 1.210224\n",
      "Test accuracy: 269/466 (57.000%)\tLoss: 21.796858\n",
      "Train Epoch: 14 [0/409 (0%)]\tLoss: 1.305803\n",
      "Train Epoch: 14 [160/409 (39%)]\tLoss: 1.017796\n",
      "Train Epoch: 14 [320/409 (78%)]\tLoss: 1.290572\n",
      "Test accuracy: 271/466 (58.000%)\tLoss: 21.126152\n",
      "Train Epoch: 15 [0/409 (0%)]\tLoss: 1.104129\n",
      "Train Epoch: 15 [160/409 (39%)]\tLoss: 1.232841\n",
      "Train Epoch: 15 [320/409 (78%)]\tLoss: 1.237332\n",
      "Test accuracy: 270/466 (57.000%)\tLoss: 20.704823\n",
      "Train Epoch: 16 [0/409 (0%)]\tLoss: 1.245996\n",
      "Train Epoch: 16 [160/409 (39%)]\tLoss: 1.279669\n",
      "Train Epoch: 16 [320/409 (78%)]\tLoss: 1.345812\n",
      "Test accuracy: 274/466 (58.000%)\tLoss: 20.733299\n",
      "Train Epoch: 17 [0/409 (0%)]\tLoss: 1.308337\n",
      "Train Epoch: 17 [160/409 (39%)]\tLoss: 1.247543\n",
      "Train Epoch: 17 [320/409 (78%)]\tLoss: 1.096051\n",
      "Test accuracy: 270/466 (57.000%)\tLoss: 20.812637\n",
      "Train Epoch: 18 [0/409 (0%)]\tLoss: 1.034553\n",
      "Train Epoch: 18 [160/409 (39%)]\tLoss: 1.185789\n",
      "Train Epoch: 18 [320/409 (78%)]\tLoss: 1.374796\n",
      "Test accuracy: 276/466 (59.000%)\tLoss: 21.063160\n",
      "Train Epoch: 19 [0/409 (0%)]\tLoss: 1.241611\n",
      "Train Epoch: 19 [160/409 (39%)]\tLoss: 1.279915\n",
      "Train Epoch: 19 [320/409 (78%)]\tLoss: 1.490440\n",
      "Test accuracy: 267/466 (57.000%)\tLoss: 21.236492\n",
      "Train Epoch: 20 [0/409 (0%)]\tLoss: 1.333402\n",
      "Train Epoch: 20 [160/409 (39%)]\tLoss: 1.241663\n",
      "Train Epoch: 20 [320/409 (78%)]\tLoss: 1.354249\n",
      "Test accuracy: 269/466 (57.000%)\tLoss: 21.773600\n",
      "Train Epoch: 21 [0/409 (0%)]\tLoss: 1.228890\n",
      "Train Epoch: 21 [160/409 (39%)]\tLoss: 1.258505\n",
      "Train Epoch: 21 [320/409 (78%)]\tLoss: 1.238421\n",
      "Test accuracy: 261/466 (56.000%)\tLoss: 21.539772\n",
      "Train Epoch: 22 [0/409 (0%)]\tLoss: 1.368845\n",
      "Train Epoch: 22 [160/409 (39%)]\tLoss: 0.983214\n",
      "Train Epoch: 22 [320/409 (78%)]\tLoss: 1.209478\n",
      "Test accuracy: 276/466 (59.000%)\tLoss: 22.710890\n",
      "Train Epoch: 23 [0/409 (0%)]\tLoss: 1.249209\n",
      "Train Epoch: 23 [160/409 (39%)]\tLoss: 1.253046\n",
      "Train Epoch: 23 [320/409 (78%)]\tLoss: 1.303093\n",
      "Test accuracy: 276/466 (59.000%)\tLoss: 21.921190\n",
      "Train Epoch: 24 [0/409 (0%)]\tLoss: 1.321909\n",
      "Train Epoch: 24 [160/409 (39%)]\tLoss: 1.085027\n",
      "Train Epoch: 24 [320/409 (78%)]\tLoss: 1.407130\n",
      "Test accuracy: 274/466 (58.000%)\tLoss: 21.465889\n",
      "Train Epoch: 25 [0/409 (0%)]\tLoss: 1.130708\n",
      "Train Epoch: 25 [160/409 (39%)]\tLoss: 1.081539\n",
      "Train Epoch: 25 [320/409 (78%)]\tLoss: 1.161735\n",
      "Test accuracy: 268/466 (57.000%)\tLoss: 23.806585\n",
      "Train Epoch: 26 [0/409 (0%)]\tLoss: 1.267008\n",
      "Train Epoch: 26 [160/409 (39%)]\tLoss: 1.470872\n",
      "Train Epoch: 26 [320/409 (78%)]\tLoss: 1.295143\n",
      "Test accuracy: 271/466 (58.000%)\tLoss: 22.425056\n",
      "Train Epoch: 27 [0/409 (0%)]\tLoss: 1.174222\n",
      "Train Epoch: 27 [160/409 (39%)]\tLoss: 1.245334\n",
      "Train Epoch: 27 [320/409 (78%)]\tLoss: 1.247381\n",
      "Test accuracy: 278/466 (59.000%)\tLoss: 21.706137\n",
      "Train Epoch: 28 [0/409 (0%)]\tLoss: 1.038660\n",
      "Train Epoch: 28 [160/409 (39%)]\tLoss: 1.359349\n",
      "Train Epoch: 28 [320/409 (78%)]\tLoss: 1.357397\n",
      "Test accuracy: 273/466 (58.000%)\tLoss: 21.801283\n",
      "Train Epoch: 29 [0/409 (0%)]\tLoss: 1.211505\n",
      "Train Epoch: 29 [160/409 (39%)]\tLoss: 1.104733\n",
      "Train Epoch: 29 [320/409 (78%)]\tLoss: 1.116541\n",
      "Test accuracy: 275/466 (59.000%)\tLoss: 21.485579\n",
      "Train Epoch: 30 [0/409 (0%)]\tLoss: 1.108467\n",
      "Train Epoch: 30 [160/409 (39%)]\tLoss: 1.251131\n",
      "Train Epoch: 30 [320/409 (78%)]\tLoss: 1.210016\n",
      "Test accuracy: 278/466 (59.000%)\tLoss: 22.308573\n",
      "Train Epoch: 31 [0/409 (0%)]\tLoss: 1.070322\n",
      "Train Epoch: 31 [160/409 (39%)]\tLoss: 0.991668\n",
      "Train Epoch: 31 [320/409 (78%)]\tLoss: 1.268738\n",
      "Test accuracy: 267/466 (57.000%)\tLoss: 21.271648\n",
      "Train Epoch: 32 [0/409 (0%)]\tLoss: 1.361594\n",
      "Train Epoch: 32 [160/409 (39%)]\tLoss: 1.156994\n",
      "Train Epoch: 32 [320/409 (78%)]\tLoss: 1.274622\n",
      "Test accuracy: 273/466 (58.000%)\tLoss: 23.174904\n",
      "Train Epoch: 33 [0/409 (0%)]\tLoss: 1.328582\n",
      "Train Epoch: 33 [160/409 (39%)]\tLoss: 1.350842\n",
      "Train Epoch: 33 [320/409 (78%)]\tLoss: 1.117530\n",
      "Test accuracy: 276/466 (59.000%)\tLoss: 22.689346\n",
      "Train Epoch: 34 [0/409 (0%)]\tLoss: 1.131179\n",
      "Train Epoch: 34 [160/409 (39%)]\tLoss: 1.151722\n",
      "Train Epoch: 34 [320/409 (78%)]\tLoss: 1.159303\n",
      "Test accuracy: 278/466 (59.000%)\tLoss: 22.685106\n",
      "Train Epoch: 35 [0/409 (0%)]\tLoss: 1.184222\n",
      "Train Epoch: 35 [160/409 (39%)]\tLoss: 1.191587\n",
      "Train Epoch: 35 [320/409 (78%)]\tLoss: 1.161977\n",
      "Test accuracy: 273/466 (58.000%)\tLoss: 23.090851\n",
      "Train Epoch: 36 [0/409 (0%)]\tLoss: 1.187063\n",
      "Train Epoch: 36 [160/409 (39%)]\tLoss: 1.261647\n",
      "Train Epoch: 36 [320/409 (78%)]\tLoss: 0.937598\n",
      "Test accuracy: 276/466 (59.000%)\tLoss: 22.155079\n",
      "Train Epoch: 37 [0/409 (0%)]\tLoss: 1.056216\n",
      "Train Epoch: 37 [160/409 (39%)]\tLoss: 0.941186\n",
      "Train Epoch: 37 [320/409 (78%)]\tLoss: 1.146041\n",
      "Test accuracy: 276/466 (59.000%)\tLoss: 22.579929\n",
      "Train Epoch: 38 [0/409 (0%)]\tLoss: 1.034956\n",
      "Train Epoch: 38 [160/409 (39%)]\tLoss: 1.214762\n",
      "Train Epoch: 38 [320/409 (78%)]\tLoss: 1.127773\n",
      "Test accuracy: 278/466 (59.000%)\tLoss: 23.779961\n",
      "Train Epoch: 39 [0/409 (0%)]\tLoss: 1.329664\n",
      "Train Epoch: 39 [160/409 (39%)]\tLoss: 1.145065\n",
      "Train Epoch: 39 [320/409 (78%)]\tLoss: 0.958838\n",
      "Test accuracy: 274/466 (58.000%)\tLoss: 22.593414\n",
      "Train Epoch: 40 [0/409 (0%)]\tLoss: 1.197903\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 40 [160/409 (39%)]\tLoss: 1.261556\n",
      "Train Epoch: 40 [320/409 (78%)]\tLoss: 1.354372\n",
      "Test accuracy: 277/466 (59.000%)\tLoss: 24.334568\n",
      "Train Epoch: 41 [0/409 (0%)]\tLoss: 1.111304\n",
      "Train Epoch: 41 [160/409 (39%)]\tLoss: 1.070910\n",
      "Train Epoch: 41 [320/409 (78%)]\tLoss: 1.095835\n",
      "Test accuracy: 267/466 (57.000%)\tLoss: 24.175266\n",
      "Train Epoch: 42 [0/409 (0%)]\tLoss: 0.976336\n",
      "Train Epoch: 42 [160/409 (39%)]\tLoss: 1.245404\n",
      "Train Epoch: 42 [320/409 (78%)]\tLoss: 0.942188\n",
      "Test accuracy: 278/466 (59.000%)\tLoss: 23.328606\n",
      "Train Epoch: 43 [0/409 (0%)]\tLoss: 0.958407\n",
      "Train Epoch: 43 [160/409 (39%)]\tLoss: 1.037192\n",
      "Train Epoch: 43 [320/409 (78%)]\tLoss: 1.298438\n",
      "Test accuracy: 279/466 (59.000%)\tLoss: 22.779661\n",
      "Train Epoch: 44 [0/409 (0%)]\tLoss: 1.148725\n",
      "Train Epoch: 44 [160/409 (39%)]\tLoss: 1.160601\n",
      "Train Epoch: 44 [320/409 (78%)]\tLoss: 1.109970\n",
      "Test accuracy: 269/466 (57.000%)\tLoss: 21.793468\n",
      "Train Epoch: 45 [0/409 (0%)]\tLoss: 0.980289\n",
      "Train Epoch: 45 [160/409 (39%)]\tLoss: 0.900421\n",
      "Train Epoch: 45 [320/409 (78%)]\tLoss: 1.093245\n",
      "Test accuracy: 276/466 (59.000%)\tLoss: 25.503937\n",
      "Train Epoch: 46 [0/409 (0%)]\tLoss: 1.037239\n",
      "Train Epoch: 46 [160/409 (39%)]\tLoss: 1.232525\n",
      "Train Epoch: 46 [320/409 (78%)]\tLoss: 1.180125\n",
      "Test accuracy: 271/466 (58.000%)\tLoss: 23.834141\n",
      "Train Epoch: 47 [0/409 (0%)]\tLoss: 1.133932\n",
      "Train Epoch: 47 [160/409 (39%)]\tLoss: 1.191573\n",
      "Train Epoch: 47 [320/409 (78%)]\tLoss: 1.306989\n",
      "Test accuracy: 264/466 (56.000%)\tLoss: 27.798866\n",
      "Train Epoch: 48 [0/409 (0%)]\tLoss: 1.018558\n",
      "Train Epoch: 48 [160/409 (39%)]\tLoss: 1.050951\n",
      "Train Epoch: 48 [320/409 (78%)]\tLoss: 1.348587\n",
      "Test accuracy: 279/466 (59.000%)\tLoss: 22.903942\n",
      "Train Epoch: 49 [0/409 (0%)]\tLoss: 1.023642\n",
      "Train Epoch: 49 [160/409 (39%)]\tLoss: 1.365415\n",
      "Train Epoch: 49 [320/409 (78%)]\tLoss: 0.894580\n",
      "Test accuracy: 271/466 (58.000%)\tLoss: 23.739491\n",
      "Model is saved!\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "   main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ---------------- Report about the quantity of data  -------------------- \n",
      "The total quantity of pairs used as data is: 1280\n",
      "The number of different people in set is: 110\n",
      "The number of pictures per person is between: 1 and 18\n",
      "The average number of pictures per person is: 6.245454545454545\n",
      " ------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.Face_DS at 0x10e600f60>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# All the data \n",
    "trans = transforms.Compose([transforms.CenterCrop(28), transforms.ToTensor(), transforms.Normalize((0.5,), (1.0,))]) # If applied, a dimensional error is raised \n",
    "archive = zipfile.ZipFile(ZIP_PATH, 'r')\n",
    "all_file_names = archive.namelist()\n",
    "Face_DS(file_names=all_file_names, transform=trans)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
